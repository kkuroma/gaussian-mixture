\section{Expectation-Maximization Algorithms}
This section aims to define a class of algorithms known as the Expectation-Maximization (EM) algorithm, based off Andrew Ng's lecture notes of CS229 from Stanford University \cite{em_notes}. These algorithms are designed to provide an approximate solution to MLE problems where, because of an unknown set of ``latent'' random variables, don't lead to a closed form solution.
\begin{definition}[Distribution Learning with Hidden Latent Variables]
    Suppose there exists 
    \begin{itemize}
        \item A distribution $\mathcal{P}$ from $\R^d$ with a joint pdf $p$ and an unknown parameter $\theta_*$
        \item A distribution $\mathcal{Q}_*$ from $\R^s$ with a pdf $q_*$.
    \end{itemize}
    Note that $\mathcal{P}$ and $\mathcal{Q}_*$ can either be explicitly given or not. Hidden behind the scenes, we sample IID:
    \begin{itemize}
        \item For $i=1,2,\dots,n$, $\mathbf{z}_i\sim\mathcal{Q}_*$ (that is, following the pdf of $q_*(\mathbf{z}_i)$)
        \item For $i=1,2,\dots,n$, $\mathbf{x}_i|\mathbf{z}_i\sim\mathcal{P}_{\theta_*}$ (that is, following the pdf of $p(\mathbf{x}_i|\mathbf{z}_i;\theta_*)$)
    \end{itemize}
    Given access to only $\mathbf{x}_i$ for $i=1,2,\dots,n$, find an estimate for $\theta_*$ and $q_*$ (the latter if not explicitly given).
\end{definition}
\subsection{The Evidence Lower Bound (ELBO)}
Like the expression for Gaussian Mixture Models, we may model the likelihood of $\theta$ and $q$ as follows:
\begin{align*}
        f(\theta,q) 
        &= \prod_{i=1}^n \int p(\mathbf{x}_i,\mathbf{z};\theta)d\mathbf{z}\\
        &= \prod_{i=1}^n \int q(\mathbf{z}) p(\mathbf{x}_i|\mathbf{z};\theta)d\mathbf{z}\\
        &= \prod_{i=1}^n \E_\mathbf{z}\left[p(\mathbf{x}_i|\mathbf{z};\theta)\right]\\
        g(\theta,q) 
        &= \log(f(\theta,q)) \\
        &= \sum_{i=1}^n \log\left(\E_\mathbf{z}\left[p(\mathbf{x}_i|\mathbf{z};\theta)\right]\right) \quad (*) \\
        &\ge \sum_{i=1}^n \E_\mathbf{z}\left[\log\left(p(\mathbf{x}_i|\mathbf{z};\theta)\right)\right] \quad (**) \\
        &= \sum_{i=1}^n \int q(\mathbf{z}) \log(p(\mathbf{x}_i|\mathbf{z};\theta)) d\mathbf{z}
\end{align*}
We're able to go from $(*)$ to $(**)$ using Jensen's inequality, as $\log$ is a convex function. This allows us to minimize the lower bound of $g(\theta,q)$ which indirectly optimizes the log-likelihood itself. This lower bound term is therefore dubbed the \textbf{evidence lower bound} (ELBO):
\[
\text{ELBO}(\mathbf{x};\theta,q) := \int q(\mathbf{z}) \log(p(\mathbf{x}|\mathbf{z};\theta)) d\mathbf{z}
\]
\textbf{Finding the optimal prior:} For the tightest bound with Jensen inequality to hold, the expectation must be taken over a constant, meaning $p(\mathbf{x}|\mathbf{z};\theta)=c$ over some constant $c$. By Bayes' law,
\begin{align*}
    p(\mathbf{x}|\mathbf{z};\theta)&=c\\
    \frac{p(\mathbf{x},\mathbf{z};\theta)}{q(\mathbf{z})}&=c\\
    q(\mathbf{z}) \propto p(\mathbf{x},\mathbf{z};\theta)
\end{align*}
And since $\int_{\mathbf{z}} q(\mathbf{z}) d\mathbf{z}=1$,
\begin{align*}
    q(\mathbf{z}) \
    &= \frac{q(\mathbf{z})}{\int_{\mathbf{z}} q(\mathbf{z}) d\mathbf{z}} \\
    &= \frac{p(\mathbf{x},\mathbf{z};\theta)}{\int_{\mathbf{z}} p(\mathbf{x},\mathbf{z};\theta)} \\
    &= \frac{p(\mathbf{x},\mathbf{z};\theta)}{p(\mathbf{x},\theta)} \\
    &= p(\mathbf{z}|\mathbf{x};\theta)
\end{align*}
Therefore, $p(\mathbf{z}|\mathbf{x};\theta)$ is the optimal choice for $q(\mathbf{z})$. Notice that this choice is dependent on $\mathbf{x}$. As such, for each $\mathbf{x}_i$, a choice $q_i$ is made as $p(\mathbf{z}|\mathbf{x}_i;\theta)$.
\begin{definition}[EM Learning Objective]
    We may isolate $\theta$ from the expression using the optimal $q_i$ as follows:
    \[
        \ell(\theta) = \sum_{i=1}^n g(\theta,q_i) \ge \sum_{i=1}^n \text{ELBO}(\mathbf{x}_i;\theta,q_i) = \sum_{i=1}^n \int q_i(\mathbf{z}) \log(p(\mathbf{x}|\mathbf{z};\theta)) d\mathbf{z} \quad\text{when}\quad q_i(\mathbf{z})=p(\mathbf{z}|\mathbf{x}_i;\theta)
    \]
\end{definition}
\subsection{Defining EM Algorithms}
The EM algorithm aims to minimize the learning objective defined above, which, if we recall, is the lower bound of the log-likelihood of a parameter $\theta$, given $q_i$ are chosen optimally.
\begin{definition}[EM Algorithm]
    The algorithm takes the scenario defined in the ``Distribution Learning with Hidden Latent Variables'' problem and carries the following steps
    \begin{itemize}
        \item \textbf{Expectation (E) Step:} compute $q_i^{(t)}(\mathbf{z})=p(\mathbf{z}|\mathbf{x}_i;\theta^{(t)})$ for $i=1,2,\dots,n$
        \item \textbf{Maximization (M) Step:} compute
        \[
            \theta^{(t+1)} = \arg\max_{\theta} \sum_{i=1}^n \text{ELBO}(\mathbf{x}_i;\theta,q_i^{(t)}) = \arg\max_{\theta} \sum_{i=1}^n \int q_i^{(t)}(\mathbf{z}) \log(p(\mathbf{x}|\mathbf{z};\theta)) d\mathbf{z}
        \]
    \end{itemize}
    With an arbitrary initial value for $\theta^{(0)}$, the algorithm repeats the E and M steps until convergence.
\end{definition}
A primary concern with such an algorithm is the convergence, which will promptly be proven
\begin{theorem}[EM Algorithm Convergence]
    The EM Algorithm defined above updates $\ell(\theta^{(t)})$ monotonically, that is, $\ell(\theta^{(t+1)})\ge \ell(\theta^{(t)})$.
\end{theorem}
\begin{proof}
    First, recall the the ELBO is defined such that $\ell(\theta) \ge \sum_{i=1}^n \text{ELBO}(\mathbf{x}_i;\theta,q_i)$ for any prior $q_i$. Furthermore, the choice for $q_i^{(t)}$ was made to ensure that Jensen's inequality holds with equality, meaning $\ell(\theta^{(t)})=\sum_{i=1}^n \text{ELBO}(\mathbf{x}_i;\theta^{(t)},q_i^{(t)})$. Therefore,
    \begin{align*}
        \ell(\theta^{(t+1)})
        &\ge \sum_{i=1}^n \text{ELBO}(\mathbf{x}_i;\theta^{(t+1)},q_i^{(t)}) &\quad\text{ELBO's definition} \\
        &\ge \sum_{i=1}^n \text{ELBO}(\mathbf{x}_i;\theta^{(t)},q_i^{(t)}) &\quad\theta^{(t+1)}\text{'s definition} \\
        &= \ell(\theta^{(t)}) &\quad\text{Jensen's Inequality Equality Case}
    \end{align*}
    As desired.
\end{proof}
\subsection{EM Algorithm for Gaussian Mixture Models}
We revisit the problem of Gaussian Mixture Models, now armed with the knowledge of the EM algorithm. We first note that the Gaussian Mixture Model problem is a specific case of the ``Distribution Learning with Hidden Latent Variables'' problem defined earlier. In particular, $\theta={\{\boldsymbol{\mu}_j,\Sigma_j\}}_{j=1}^k$, $p$ is the Gaussian Mixture pdf, and $q$ is the proportion of each possible class of $z$ showing up. The EM Algorithm becomes
\begin{itemize}
    \item \textbf{Expectation (E) Step:} $q_i^{(t)}(z)=p(z|\mathbf{x}_i;\theta^{(t)})$ for $i=1,2,\dots,n$
    \item \textbf{Maximization (M) Step:} We first use Bayes's rule to re-write the terms
    \begin{align*}
        \theta^{(t+1)} 
        &= \arg\max_{\theta} \sum_{i=1}^n \sum_{z=1}^k q_i^{(t)}(z) \log(p(\mathbf{x}|z;\boldsymbol{\mu}_z,\Sigma_z,\phi_z)) \\
        &= \arg\max_{\theta} \sum_{i=1}^n \sum_{z=1}^k q_i^{(t)}(z) \log\left(\frac{p(\mathbf{x},z;\boldsymbol{\mu}_z,\Sigma_z,\phi_z)}{q_i^{(t)}(z)}\right) \\
        &= \arg\max_{\theta} \sum_{i=1}^n \sum_{z=1}^k q_i^{(t)}(z) \log\left(\frac{p(\mathbf{x}|z;\boldsymbol{\mu}_z,\Sigma_z)p(z,\phi_z)}{q_i^{(t)}(z)}\right) \\
        &= \arg\max_{\theta} \sum_{i=1}^n \sum_{z=1}^k q_i^{(t)}(z) \log\left(\frac{p(\mathbf{x}|z;\boldsymbol{\mu}_z,\Sigma_z)\phi_z}{q_i^{(t)}(z)}\right)
    \end{align*}
    therefore, taking the log and removing constant terms
    \[
        \theta^{(t+1)} = \arg\max_{\theta} \sum_{i=1}^n \sum_{z=1}^k q_i^{(t)}(z) 
        \left(
            -\frac{n}{2} \log |\Sigma_z|
            - \frac{1}{2} \sum_{i=1}^n (\mathbf{x}_i - \boldsymbol{\mu}_z)^\top \Sigma_z^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_z)
            + \log(\phi_z)
        \right)
    \]
\end{itemize}
Notice that the optimization terms for $\boldsymbol{\mu}_z$ and $\Sigma_z$ is a linear combination of several log-likelihood terms of regular Gaussian fitting. Indeed, we are able to use the results from the prerequisites section to conclude that for $z=1,2,\dots,k$
\[
\boldsymbol{\mu}_z^{(t)} = \frac{\sum_{i=1}^n q_i^{(t)}(z) \mathbf{x}_i}{\sum_{i=1}^n q_i^{(t)}(z)}
\text{ and }
\Sigma_z^{(t)}  = \frac{\sum_{i=1}^n q_i^{(t)}(z) (\mathbf{x}_i-\boldsymbol{\mu}_z)(\mathbf{x}_i-\boldsymbol{\mu}_z)^\top}{\sum_{i=1}^n q_i^{(t)}(z)}
\]
Furthermore, the optimization terms $\phi_z$ are a linear combination of several log-likelihood terms of the categorical sampling. Similarly, we can use the results the prerequisites section to conclude that for $z=1,2,\dots,k$
\[
\phi_z^{(t)}  = \frac{1}{n} \sum_{i=1}^n q_i^{(t)}(z)
\]
\textbf{Conclusion}: The Gaussian Mixture problem can be seen as a special case for the previously defined ``Distribution Learning with Hidden Latent Variables'' problem. Using the EM algorithm, we're able to derive an iterative procedure that approximates $\boldsymbol{\mu}_z,\Sigma_z,\phi_z$ in a closed form.
\subsection{Extending the EM Algorithm}
The Variational Autoencoder (VAE) \cite{vae} is a popular extension of the EM Algorithm. Particularly, the \textbf{E-step} of most EM algorithm involves computing $q_i^{(t)}(\mathbf{z})=p(\mathbf{z}|\mathbf{x}_i;\theta^{(t)})$ for $i=1,2,\dots,n$, which is often an \textbf{intractable problem} for most distributions. The VAE replaces the posterior $p(\mathbf{z}|\mathbf{x}_i;\theta^{(t)})$ by a learnable distribution $q$ parametrized by $\phi$. Commonly, the two distributions $p(\mathbf{x}|\mathbf{z};\theta)$ (\textbf{Decoder}) and $q(\mathbf{z}|\mathbf{x};\phi)$ (\textbf{Encoder}) are learned through a neural network with parameters $\theta$ and $\phi$ respectively. Revisiting the ELBO, now reparametrized with $\theta$ and $\phi$ as $q$ is parametrized on the latter,
\begin{align*}
    \text{ELBO}(\mathbf{x},\theta,\phi)
    &= \int q(\mathbf{z}|\mathbf{x};\phi) \log\left(\frac{p(\mathbf{x},\mathbf{z};\theta)}{q(\mathbf{z}|\mathbf{x};\phi)}\right) d\mathbf{z} \\
    &= \int q(\mathbf{z}|\mathbf{x};\phi) \log\left(\frac{p(\mathbf{x}|\mathbf{z};\theta)p(\mathbf{z};\theta)}{q(\mathbf{z}|\mathbf{x};\phi)}\right) d\mathbf{z} \\
    &= \int q(\mathbf{z}|\mathbf{x};\phi) \log(p(\mathbf{x}|\mathbf{z};\theta)) d\mathbf{z} - \int q(\mathbf{z}|\mathbf{x};\phi) \log\left(\frac{q(\mathbf{z}|\mathbf{x};\phi)}{p(\mathbf{z};\theta)}\right) d\mathbf{z}
\end{align*}
\begin{itemize}
    \item The first term $\int q(\mathbf{z}|\mathbf{x};\phi) \log(p(\mathbf{x}|\mathbf{z};\theta)) d\mathbf{z}=\E_{\mathbf{z}\sim q_{\phi}}[\log(p(\mathbf{x}|\mathbf{z};\theta))]$ can be derived further under the assumption that $p(\mathbf{x}|\mathbf{z};\theta)$ is Gaussian of some mean $\mu_{\theta}(\mathbf{z})$ parameterized on $\theta$ and a constant diagonal covariance $\sigma^2 \mathbf{I}_{d\times d}$
    \begin{align*}
        \log(p(\mathbf{x}|\mathbf{z};\theta))
        &= \log\left( \frac{1}{\sigma^d (2\pi)^{d/2}} \exp\left( -\frac{\|\mathbf{x} - \mu_\theta(\mathbf{z})\|_2^2}{2\sigma^2} \right) \right) \\
        &= -\|\mathbf{x} - \mu_\theta(\mathbf{z})\|_2^2 + C
    \end{align*}
    when the constant terms are grouped to $C$. As such, optimizing this term is equivalent to minimizing $\E[\|\mathbf{x} - \mu_\theta(\mathbf{z})\|_2^2]$, the euclidean difference between the data point $\mathbf{x}$ and its reconstruction $\mu_\theta(\mathbf{z})$. During training, this is optimized stochastically with monte-carlo samples.
    \item The second term $\int q(\mathbf{z}|\mathbf{x};\phi) \log\left(\frac{q(\mathbf{z}|\mathbf{x};\phi)}{p(\mathbf{z};\theta)}\right) d\mathbf{z}$ is the KL divergence between the two distributions $q(\mathbf{z}|\mathbf{x};\phi)$ and $p(\mathbf{z};\theta)$. We first assume $p(\mathbf{z};\theta)$ follows a standard $s-$dimensional Gaussian pdf. We then assume that $q(\mathbf{z}|\mathbf{x};\phi)$ follows $\normal(\mu_{\phi}(\mathbf{x}),\text{diag}(\sigma_{\phi}^2(\mathbf{x})))$ (when $\text{diag}(\mathbf{v})$ takes $\mathbf{v}\in\R^s$ and returns an $s\times s$ diagonal matrix with entries from the basis coefficient of $\mathbf{v}$). Using results from the prerequisites section, this KL divergence can be expressed as follows:
    \[
        \mathrm{KL}(q(\mathbf{z}|\mathbf{x};\phi) \| p(\mathbf{z};\theta)) = \frac{1}{2} \sum_{i=1}^s \left( \sigma_{\phi,i}^2(\mathbf{x}) + \mu_{\phi,i}^2(\mathbf{x}) - 1 - \log \sigma_{\phi,i}^2(\mathbf{x}) \right)
    \]
    \textbf{Remark:} this is known as the reparametrization trick; the encoder $q$ produces two vectors $\mu_{\phi}(\mathbf{x})$ and $\sigma_{\phi}^2(\mathbf{x})$ to parameterize the Gaussian distribution $\text{z}$ is then sampled from. Oftentimes, the log-variance is predicted instead to prevent negative-valued predictions.
\end{itemize}
\textbf{Conclusion} the VAE provides an alternative to EM algorithm's \textbf{E-step}, which involves an often intractable term $p(\mathbf{z}|\mathbf{x}_i;\theta^{(t)})$. Instead, the VAE learns two distributions $p(\mathbf{x}|\mathbf{z};\theta)$ (\textbf{Decoder}) and $q(\mathbf{z}|\mathbf{x};\phi)$ (\textbf{Encoder}) simultaneously to maximize the ELBO. By several assumptions of $p(\mathbf{x}|\mathbf{z};\theta)$, $q(\mathbf{z}|\mathbf{x};\phi)$, and $p(\mathbf{z};\theta)$ normality, the ELBO simplifies to a sum of the reconstruction loss and KL divergence
\[
    \hat{\theta},\hat{\phi} = \arg\min_{\theta,\phi} \E[\|\mathbf{x} - \mu_\theta(\mathbf{z})\|_2^2] + \frac{1}{2} \sum_{i=1}^s \left( \sigma_{\phi,i}^2(\mathbf{x}) + \mu_{\phi,i}^2(\mathbf{x}) - 1 - \log \sigma_{\phi,i}^2(\mathbf{x}) \right)
\]
which could be learned through (stochastic) gradient descent. Unlike the EM algorithm, the VAE is only able to improve the lower bound (ELBO) of the log-likelihood, meaning convergence could not be guaranteed.