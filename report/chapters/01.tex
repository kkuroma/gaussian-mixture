\section{Gaussian Mixture Model}
\subsection{Defining Gaussian Mixture}
\begin{definition}[Gaussian Mixture Problem]
    For $j=1,2,\dots,k$ suppose there exists some fixed, hidden parameters $\phi_j \in \R,\boldsymbol{\mu}_j \in \R^d,\Sigma_j \in \R^{d\times d}$ such that $\sum_{j=1}^k \phi_j = 1$ and $\Sigma_j$ is a valid covariance matrix, we consider the following procedure
    \begin{itemize}
        \item For $i=1,2,\dots,n$ sample IID $z_i$ from $\{1,2,\dots,k\}$ such that $\p(z_i)=\phi_j$
        \item For $i=1,2,\dots,n$ sample IID $\textbf{x}_i$ from $\normal(\boldsymbol{\mu}_{z_i},\Sigma_{z_i})$
    \end{itemize}
    There are two versions of the problems:
    \begin{itemize}
        \item \textbf{Supervised}: for $i=1,2,\dots,n$ we are given $\textbf{x}_i,z_i$. Find the estimate for $\phi_j,\boldsymbol{\mu}_j,\Sigma_j$
        \item \textbf{Unsupervised}: for $i=1,2,\dots,n$ we are given only $\textbf{x}_i$. Find the estimate for $\phi_j,\boldsymbol{\mu}_j,\Sigma_j$
    \end{itemize}
\end{definition}
This is assuming that there exists $k$ different Gaussian clusters, and $\mathbf{x}_i$ is sampled from one of them with probability specified by $\phi_j$. Notice that the supervised version of the problem essentially boils down to running the regular Multivariate Gaussian problem for each class $z_i$ can take. We are more interested in the unsupervised problem, as the MLE could not be solved in a closed form.
\subsection{Potential Applications of Gaussian Mixture}
We present two applications of the unsupervised version of the Gaussian Mixture defined above. The exact implementations will be explained in later sessions.
\begin{itemize}
    \item \textbf{Unsupervised classification}: the resulting estimate for the Gaussian clusters' mean and covariance allows estimates for the unknown label $z_i$ to be retrieved from $\textbf{x}_i$.
    \item \textbf{Conditional generation}: the Gaussian Mixture model allows data outside of the given set to be sampled from the resulting Gaussian distributions, conditioned on one of the possible classes.
\end{itemize}
\subsection{Gaussian Mixture as MLE Problem}
Given access to only the data vectors $\textbf{x}_i$ for $i=1,2,\dots,n$, one may model the likelihood of the parameters ${\{\phi_j,\boldsymbol{\mu}_j,\Sigma_j\}}_{j=1}^k$ as follows:
\begin{align*}
    f({\{\phi_j,\boldsymbol{\mu}_j,\Sigma_j\}}_{j=1}^k) 
    &= \prod_{i=1}^n \sum_{z=1}^k \p(Z=k)p(\mathbf{x}_n;\boldsymbol{\mu}_k,\Sigma_k) \\
    &= \prod_{i=1}^n \sum_{z=1}^k \frac{\phi_z}{(2\pi)^{d/2}|\Sigma_z|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{x}_i - \boldsymbol{\mu}_z)^\top \Sigma_z^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_z) \right)
\end{align*}
This results in a double summation once a logarithm is taken, particularly with the term inside logarithm being a summation. While a closed form MLE solution doesn't exist for this expression, the use of the EM algorithm explained over the next sections will help us iteratively solve for a good approximate.