{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db6d579b",
   "metadata": {},
   "source": [
    "# Variational Autoencoder\n",
    "\n",
    "The variational autoencoder (VAE) aims to learn the distribution of a dataset $\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_n$ given they are sampled conditioned on a latent $\\mathbf{z}$ sampled as a standard Gaussian vector. Below, we show a basic implementation of a VAE that will be the basis for unsupervised generation down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2ad3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "#for consistency, all seeds are set to 69420\n",
    "seed = 69420\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c19117",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder aims to model $q(\\mathbf{z}|\\mathbf{x};\\phi)$. This network produces the mean and covariance using $\\mathbf{x}$, from where $\\mathbf{z}$ can be sampled. The implementation uses convolutional neural network components to map $\\mathbf{x}$ to the means and covariances. A trick used here is to sample the covariance from the logarithmic space to ensure positivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7fbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_size, img_channel, components_num):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(img_channel, 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        #compute the mean and covariance for all components\n",
    "        self.mu_fc = nn.Conv2d(256, components_num * latent_size, 2, 1, 0, bias=False)\n",
    "        self.sigma_fc = nn.Conv2d(256, components_num * latent_size, 2, 1, 0, bias=False) #include/not include number of components?\n",
    "        self.weights_fc = nn.Conv2d(256, components_num, 2, 1, 0, bias=False)\n",
    "        self.kl = 0\n",
    "        \n",
    "    def forward(self, x, latent_size):\n",
    "        encoded = self.encoder(x)\n",
    "        mu = self.mu_fc(encoded).view(encoded.size(0), -1, latent_size) #mean\n",
    "        sigma = torch.exp(self.sigma_fc(encoded).view(encoded.size(0), -1, latent_size)) #covariance\n",
    "        weights = torch.softmax(self.weights_fc(encoded).view(encoded.size(0), -1, latent_size), dim=1) #weights\n",
    "\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        self.kl = self.kl_loss(mu, sigma, weights) # kl loss term\n",
    "\n",
    "        return z, mu, sigma, weights\n",
    "    \n",
    "    #reparameterization trick\n",
    "    def reparameterize(self, mu, sigma):\n",
    "        sd = torch.sqrt(sigma + 1e-8)\n",
    "        noise = torch.randn_like(sd)\n",
    "        z = mu + sd * noise\n",
    "        return z\n",
    "    \n",
    "\n",
    "    # KL divergence loss\n",
    "    def kl_loss(self, mu, sigma, weights):\n",
    "        kl_component = 0.5 * torch.sum(sigma**2 + mu**2 - torch.log(sigma) - 1)\n",
    "        kl = torch.mean(torch.sum(weights * kl_component))\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4556a68e",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "On the other hand, the decoder learns $p(\\mathbf{x}|\\mathbf{z};\\theta)$ by reconstructing the give data $x$ using information it was given from $\\mathbf{z}$. Implementation-wise, this decoder uses convolution transpose blocks to reverse the convolution of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd8693df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_size, img_channel):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.conv_transpose_block_1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_size, 256, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        self.conv_transpose_block_2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        self.conv_transpose_block_3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        self.conv_transpose_block_4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        self.conv_transpose_block_5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, img_channel, 1, 1, 0, bias=False),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_transpose_block_1(x)\n",
    "        x = self.conv_transpose_block_2(x)\n",
    "        x = self.conv_transpose_block_3(x)\n",
    "        x = self.conv_transpose_block_4(x)\n",
    "        x = self.conv_transpose_block_5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec2e55",
   "metadata": {},
   "source": [
    "## VAE training\n",
    "We optimize the ELBO stochastically using the gradient descent, which boils down the minimizing the KL divergence and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f5355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
